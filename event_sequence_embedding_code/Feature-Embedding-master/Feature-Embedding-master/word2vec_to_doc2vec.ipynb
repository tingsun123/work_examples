{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "A word embedding is an approach to provide a dense vector representation of words that capture something about their meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are an improvement over simpler bag-of-word model word encoding schemes like word counts and frequencies that result in large and sparse vectors (mostly 0 values) that describe documents but not the meaning of the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings work by using an algorithm to train a set of fixed-length dense and continuous-valued vectors based on a large corpus of text. Each word is represented by a point in the embedding space and these points are learned and moved around based on the words that surround the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is an open source Python library for natural language processing, with a focus on topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/33/33/df6cb7acdcec5677ed130f4800f67509d24dbec74a03c329fcbf6b0864f0/gensim-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (22.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 22.6MB 36kB/s eta 0:00:011  5% |█▋                              | 1.1MB 5.4MB/s eta 0:00:04    31% |██████████                      | 7.0MB 5.4MB/s eta 0:00:03    33% |██████████▋                     | 7.5MB 8.7MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/69/c92661a333f733510628f28b8282698b62cdead37291c8491f3271677c02/smart_open-1.5.7.tar.gz\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/b7/a88a67002b1185ed9a8e8a6ef15266728c2361fcb4f1d02ea331e4c7741d/boto-2.48.0-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 513kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/88/acf7e4ec3a31c0d3da51037c6c94cc93a7595d4eb04fbb01e7f4f4e2dbc5/boto3-1.7.31-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 2.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Collecting botocore<1.11.0,>=1.10.31 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/11/5c/a64d7550362200100e145519952a651e43342c848c9988f37c3403caab7a/botocore-1.10.31-py2.py3-none-any.whl (4.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.3MB 178kB/s ta 0:00:011   33% |██████████▉                     | 1.5MB 2.0MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.31->boto3->smart-open>=1.2.1->gensim)\n",
      "Collecting docutils>=0.10 (from botocore<1.11.0,>=1.10.31->boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 856kB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/b1/9e/7d/bb3d3b55c597e72617140a0638c06382a5f17283881eae163e\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.48.0 boto3-1.7.31 botocore-1.10.31 bz2file-0.98 docutils-0.14 gensim-3.4.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.5.7\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "* window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "* min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "* workers: (default 3) The number of threads to use while training.\n",
    "* sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsun04\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df=pd.read_csv('reviews.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "\n",
       "          file  \n",
       "0      0_2.txt  \n",
       "1  10000_4.txt  \n",
       "2  10001_1.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenzied']=df[\"review\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, Word2Vec expects to be given a list of sentences, each of which is a list of words. To make this data setup, we define a function to split our sentences into lists of words and then apply this within another function that splits our texts into lists of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(df['tokenzied'], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=998, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once',\n",
       " 'again',\n",
       " 'Mr.',\n",
       " 'Costner',\n",
       " 'has',\n",
       " 'dragged',\n",
       " 'out',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'for',\n",
       " 'far',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'necessary.',\n",
       " 'Aside',\n",
       " 'from',\n",
       " 'the',\n",
       " 'terrific',\n",
       " 'sea',\n",
       " 'rescue',\n",
       " 'sequences,',\n",
       " 'of',\n",
       " 'which',\n",
       " 'there',\n",
       " 'are',\n",
       " 'very',\n",
       " 'few',\n",
       " 'I',\n",
       " 'just',\n",
       " 'did',\n",
       " 'not',\n",
       " 'care',\n",
       " 'about',\n",
       " 'any',\n",
       " 'characters.',\n",
       " 'Most',\n",
       " 'us',\n",
       " 'have',\n",
       " 'ghosts',\n",
       " 'in',\n",
       " 'closet,',\n",
       " 'and',\n",
       " \"Costner's\",\n",
       " 'character',\n",
       " 'realized',\n",
       " 'early',\n",
       " 'on,',\n",
       " 'then',\n",
       " 'forgotten',\n",
       " 'until',\n",
       " 'much',\n",
       " 'later,',\n",
       " 'by',\n",
       " 'time',\n",
       " 'care.',\n",
       " 'The',\n",
       " 'we',\n",
       " 'should',\n",
       " 'really',\n",
       " 'is',\n",
       " 'cocky,',\n",
       " 'overconfident',\n",
       " 'Ashton',\n",
       " 'Kutcher.',\n",
       " 'problem',\n",
       " 'he',\n",
       " 'comes',\n",
       " 'off',\n",
       " 'as',\n",
       " 'kid',\n",
       " 'who',\n",
       " 'thinks',\n",
       " \"he's\",\n",
       " 'better',\n",
       " 'anyone',\n",
       " 'else',\n",
       " 'around',\n",
       " 'him',\n",
       " 'shows',\n",
       " 'no',\n",
       " 'signs',\n",
       " 'cluttered',\n",
       " 'closet.',\n",
       " 'His',\n",
       " 'only',\n",
       " 'obstacle',\n",
       " 'appears',\n",
       " 'to',\n",
       " 'be',\n",
       " 'winning',\n",
       " 'over',\n",
       " 'Costner.',\n",
       " 'Finally',\n",
       " 'when',\n",
       " 'well',\n",
       " 'past',\n",
       " 'half',\n",
       " 'way',\n",
       " 'point',\n",
       " 'this',\n",
       " 'stinker,',\n",
       " 'tells',\n",
       " 'all',\n",
       " \"Kutcher's\",\n",
       " 'ghosts.',\n",
       " 'We',\n",
       " 'told',\n",
       " 'why',\n",
       " 'Kutcher',\n",
       " 'driven',\n",
       " 'best',\n",
       " 'with',\n",
       " 'prior',\n",
       " 'inkling',\n",
       " 'or',\n",
       " 'foreshadowing.',\n",
       " 'No',\n",
       " 'magic',\n",
       " 'here,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'could',\n",
       " 'do',\n",
       " 'keep',\n",
       " 'turning',\n",
       " 'an',\n",
       " 'hour',\n",
       " 'in.',\n",
       " 'This',\n",
       " 'example',\n",
       " 'majority',\n",
       " 'action',\n",
       " 'films',\n",
       " 'same.',\n",
       " 'Generic',\n",
       " 'boring,',\n",
       " \"there's\",\n",
       " 'nothing',\n",
       " 'worth',\n",
       " 'watching',\n",
       " 'here.',\n",
       " 'A',\n",
       " 'complete',\n",
       " 'waste',\n",
       " 'barely-tapped',\n",
       " 'talents',\n",
       " 'Ice-T',\n",
       " 'Ice',\n",
       " 'Cube,',\n",
       " \"who've\",\n",
       " 'each',\n",
       " 'proven',\n",
       " 'many',\n",
       " 'times',\n",
       " 'that',\n",
       " 'they',\n",
       " 'capable',\n",
       " 'acting,',\n",
       " 'acting',\n",
       " 'well.',\n",
       " \"Don't\",\n",
       " 'bother',\n",
       " 'one,',\n",
       " 'go',\n",
       " 'see',\n",
       " 'New',\n",
       " 'Jack',\n",
       " 'City,',\n",
       " 'Ricochet',\n",
       " 'watch',\n",
       " 'York',\n",
       " 'Undercover',\n",
       " 'Ice-T,',\n",
       " 'Boyz',\n",
       " 'n',\n",
       " 'Hood,',\n",
       " 'Higher',\n",
       " 'Learning',\n",
       " 'Friday',\n",
       " 'Cube',\n",
       " 'real',\n",
       " 'deal.',\n",
       " \"Ice-T's\",\n",
       " 'horribly',\n",
       " 'cliched',\n",
       " 'dialogue',\n",
       " 'alone',\n",
       " 'makes',\n",
       " 'film',\n",
       " 'grate',\n",
       " 'at',\n",
       " 'teeth,',\n",
       " \"I'm\",\n",
       " 'still',\n",
       " 'wondering',\n",
       " 'what',\n",
       " 'heck',\n",
       " 'Bill',\n",
       " 'Paxton',\n",
       " 'doing',\n",
       " 'film?',\n",
       " 'And',\n",
       " 'does',\n",
       " 'always',\n",
       " 'play',\n",
       " 'exact',\n",
       " 'same',\n",
       " 'character?',\n",
       " 'From',\n",
       " 'Aliens',\n",
       " 'onward,',\n",
       " 'every',\n",
       " \"I've\",\n",
       " 'seen',\n",
       " 'playing',\n",
       " 'irritating',\n",
       " 'character,',\n",
       " 'least',\n",
       " 'his',\n",
       " 'died,',\n",
       " 'made',\n",
       " 'somewhat',\n",
       " 'gratifying...<br',\n",
       " '/><br',\n",
       " '/>Overall,',\n",
       " 'second-rate',\n",
       " 'trash.',\n",
       " 'There',\n",
       " 'countless',\n",
       " 'see,',\n",
       " 'if',\n",
       " 'you',\n",
       " 'want',\n",
       " 'Judgement',\n",
       " 'Night,',\n",
       " 'practically',\n",
       " 'carbon',\n",
       " 'copy',\n",
       " 'but',\n",
       " 'script.',\n",
       " 'thing',\n",
       " 'decent',\n",
       " 'hand',\n",
       " 'on',\n",
       " 'camera',\n",
       " '-',\n",
       " 'cinematography',\n",
       " 'almost',\n",
       " 'refreshing,',\n",
       " 'close',\n",
       " 'making',\n",
       " 'up',\n",
       " 'horrible',\n",
       " 'itself',\n",
       " 'quite.',\n",
       " '4/10.',\n",
       " 'First',\n",
       " 'hate',\n",
       " 'those',\n",
       " 'moronic',\n",
       " 'rappers,',\n",
       " \"could'nt\",\n",
       " 'act',\n",
       " 'had',\n",
       " 'gun',\n",
       " 'pressed',\n",
       " 'against',\n",
       " 'their',\n",
       " 'foreheads.',\n",
       " 'All',\n",
       " 'curse',\n",
       " 'shoot',\n",
       " 'other',\n",
       " 'like',\n",
       " \"clichÃ©'e\",\n",
       " 'version',\n",
       " 'gangsters.<br',\n",
       " '/>The',\n",
       " \"doesn't\",\n",
       " 'take',\n",
       " 'more',\n",
       " 'five',\n",
       " 'minutes',\n",
       " 'explain',\n",
       " 'going',\n",
       " 'before',\n",
       " \"we're\",\n",
       " 'already',\n",
       " 'warehouse',\n",
       " 'single',\n",
       " 'sympathetic',\n",
       " 'movie,',\n",
       " 'except',\n",
       " 'homeless',\n",
       " 'guy,',\n",
       " 'also',\n",
       " 'one',\n",
       " 'brain.<br',\n",
       " '/>Bill',\n",
       " 'William',\n",
       " 'Sadler',\n",
       " 'both',\n",
       " 'hill',\n",
       " 'billies',\n",
       " 'Sadlers',\n",
       " 'villain',\n",
       " 'gangsters.',\n",
       " \"did'nt\",\n",
       " 'right',\n",
       " 'start.<br',\n",
       " 'filled',\n",
       " 'pointless',\n",
       " 'violence',\n",
       " 'Walter',\n",
       " 'Hills',\n",
       " 'specialty:',\n",
       " 'people',\n",
       " 'falling',\n",
       " 'through',\n",
       " 'windows',\n",
       " 'glass',\n",
       " 'flying',\n",
       " 'everywhere.',\n",
       " 'pretty',\n",
       " 'plot',\n",
       " 'big',\n",
       " 'root',\n",
       " 'no-one.',\n",
       " 'Everybody',\n",
       " 'dies,',\n",
       " 'guy',\n",
       " 'everybody',\n",
       " 'get',\n",
       " 'deserve.<br',\n",
       " 'two',\n",
       " 'black',\n",
       " 'can',\n",
       " 'junkie',\n",
       " \"they're\",\n",
       " 'actors',\n",
       " 'profession,',\n",
       " 'annoying',\n",
       " 'ugly',\n",
       " 'brain',\n",
       " 'dead',\n",
       " 'rappers.<br',\n",
       " '/>Stay',\n",
       " 'away',\n",
       " 'crap',\n",
       " '48',\n",
       " 'hours',\n",
       " '1',\n",
       " '2',\n",
       " 'instead.',\n",
       " 'At',\n",
       " 'lest',\n",
       " 'characters',\n",
       " 'about,',\n",
       " 'sense',\n",
       " 'humor',\n",
       " 'cast.',\n",
       " 'Not',\n",
       " 'even',\n",
       " 'Beatles',\n",
       " 'write',\n",
       " 'songs',\n",
       " 'everyone',\n",
       " 'liked,',\n",
       " 'although',\n",
       " 'Hill',\n",
       " 'mop-top',\n",
       " 'second',\n",
       " 'none',\n",
       " 'thought',\n",
       " 'provoking',\n",
       " 'movies.',\n",
       " 'nineties',\n",
       " 'came',\n",
       " 'social',\n",
       " 'platforms',\n",
       " 'were',\n",
       " 'changing',\n",
       " 'music',\n",
       " 'film,',\n",
       " 'emergence',\n",
       " 'Rapper',\n",
       " 'turned',\n",
       " 'star',\n",
       " 'full',\n",
       " 'swing,',\n",
       " 'took',\n",
       " 'back',\n",
       " 'seat',\n",
       " \"man's\",\n",
       " 'overpowering',\n",
       " 'regional',\n",
       " 'accent',\n",
       " 'transparent',\n",
       " 'acting.',\n",
       " 'ice-t',\n",
       " 'movies',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'loved,',\n",
       " 'them',\n",
       " 'later',\n",
       " 'cringe.',\n",
       " 'firemen',\n",
       " 'basic',\n",
       " 'lives',\n",
       " 'burning',\n",
       " 'building',\n",
       " 'tenant',\n",
       " 'flames',\n",
       " 'hands',\n",
       " 'map',\n",
       " 'gold',\n",
       " 'implications.',\n",
       " 'quickly',\n",
       " 'neatly',\n",
       " 'setting',\n",
       " 'main',\n",
       " 'location.',\n",
       " 'But',\n",
       " 'fault',\n",
       " 'involved',\n",
       " 'Lame-o',\n",
       " 'performances.',\n",
       " 'Ice-t',\n",
       " 'cube',\n",
       " 'must',\n",
       " 'been',\n",
       " 'red',\n",
       " 'hot',\n",
       " 'time,',\n",
       " 'while',\n",
       " 'enjoyed',\n",
       " 'careers',\n",
       " 'my',\n",
       " 'opinion',\n",
       " 'fell',\n",
       " 'flat',\n",
       " 'movie.',\n",
       " \"It's\",\n",
       " 'ninety',\n",
       " 'ridiculously',\n",
       " 'find',\n",
       " 'yourself',\n",
       " 'locked',\n",
       " 'multiple',\n",
       " 'states',\n",
       " 'disbelief.',\n",
       " 'Now',\n",
       " 'its',\n",
       " 'documentary',\n",
       " 'so',\n",
       " 'wont',\n",
       " 'recounting',\n",
       " 'stupid',\n",
       " 'twists',\n",
       " 'many,',\n",
       " 'led',\n",
       " 'nowhere.',\n",
       " 'got',\n",
       " 'feeling',\n",
       " 'set',\n",
       " 'sord',\n",
       " 'confused',\n",
       " 'things',\n",
       " 'cuff.',\n",
       " 'enjoy',\n",
       " 'it,',\n",
       " 'involves',\n",
       " 'scene',\n",
       " 'needle',\n",
       " \"Sadler's\",\n",
       " 'huge',\n",
       " '45',\n",
       " 'pistol.',\n",
       " 'Bottom',\n",
       " 'line',\n",
       " \"domino's\",\n",
       " 'pizza.',\n",
       " 'Yeah',\n",
       " 'ill',\n",
       " 'eat',\n",
       " 'hungry',\n",
       " \"don't\",\n",
       " 'feel',\n",
       " 'cooking,',\n",
       " 'aware',\n",
       " 'tastes',\n",
       " 'crap.',\n",
       " '3',\n",
       " 'stars,',\n",
       " 'meh.',\n",
       " 'Brass',\n",
       " 'pictures',\n",
       " '(movies',\n",
       " 'fitting',\n",
       " 'word',\n",
       " 'them)',\n",
       " 'brassy.',\n",
       " 'Their',\n",
       " 'alluring',\n",
       " 'visual',\n",
       " 'qualities',\n",
       " 'reminiscent',\n",
       " 'expensive',\n",
       " 'high',\n",
       " 'class',\n",
       " 'TV',\n",
       " 'commercials.',\n",
       " 'unfortunately',\n",
       " 'feature',\n",
       " 'pretense',\n",
       " 'wanting',\n",
       " 'entertain',\n",
       " 'viewers',\n",
       " 'hours!',\n",
       " 'In',\n",
       " 'fail',\n",
       " 'miserably,',\n",
       " 'undeniable,',\n",
       " 'rather',\n",
       " 'soft',\n",
       " 'flabby',\n",
       " 'steamy,',\n",
       " 'erotic',\n",
       " 'non',\n",
       " 'withstanding.<br',\n",
       " '/>Senso',\n",
       " \"'45\",\n",
       " 'remake',\n",
       " 'Luchino',\n",
       " 'Visconti',\n",
       " 'title',\n",
       " 'Alida',\n",
       " 'Valli',\n",
       " 'Farley',\n",
       " 'Granger',\n",
       " 'lead.',\n",
       " 'original',\n",
       " 'story',\n",
       " 'senseless',\n",
       " 'love',\n",
       " 'lust',\n",
       " 'Venice',\n",
       " 'during',\n",
       " 'Italian',\n",
       " 'wars',\n",
       " 'independence.',\n",
       " 'moved',\n",
       " '19th',\n",
       " 'into',\n",
       " '20th',\n",
       " 'century,',\n",
       " '1945',\n",
       " 'exact,',\n",
       " 'Mussolini',\n",
       " 'murals,',\n",
       " 'men',\n",
       " 'shirts,',\n",
       " 'German',\n",
       " 'uniforms',\n",
       " 'tattered',\n",
       " 'garb',\n",
       " 'partisans.',\n",
       " 'window',\n",
       " 'dressing,',\n",
       " 'historic',\n",
       " 'context',\n",
       " 'completely',\n",
       " 'negligible.<br',\n",
       " '/>Anna',\n",
       " 'Galiena',\n",
       " 'plays',\n",
       " 'attractive',\n",
       " 'aristocratic',\n",
       " 'woman',\n",
       " 'falls',\n",
       " 'amoral',\n",
       " 'SS',\n",
       " 'puts',\n",
       " 'too',\n",
       " 'lipstick.',\n",
       " 'She',\n",
       " 'attractive,',\n",
       " 'versatile,',\n",
       " 'trained',\n",
       " 'actress',\n",
       " 'clearly',\n",
       " 'above',\n",
       " 'material.',\n",
       " 'Her',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'facial',\n",
       " 'expressions',\n",
       " '(signalling',\n",
       " 'boredom,',\n",
       " 'loathing,',\n",
       " 'delight,',\n",
       " 'fear,',\n",
       " '...',\n",
       " 'ecstasy)',\n",
       " 'reason',\n",
       " 'picture',\n",
       " 'stars.',\n",
       " 'endures',\n",
       " 'basically',\n",
       " 'trashy',\n",
       " 'stuff',\n",
       " 'astonishing',\n",
       " 'amount',\n",
       " 'dignity.',\n",
       " 'wish',\n",
       " 'some',\n",
       " 'good',\n",
       " 'parts',\n",
       " 'come',\n",
       " 'along',\n",
       " 'her.',\n",
       " 'deserves',\n",
       " 'it.',\n",
       " 'funny',\n",
       " 'happened',\n",
       " 'me',\n",
       " '\"Mosquito\":',\n",
       " 'hand,',\n",
       " 'hero',\n",
       " 'deaf-mute',\n",
       " 'director',\n",
       " 'totally',\n",
       " 'unable',\n",
       " 'make',\n",
       " 'understand',\n",
       " '(mutilating',\n",
       " 'mannequins...er,',\n",
       " 'excuse',\n",
       " 'me,',\n",
       " 'corpses)',\n",
       " 'images.',\n",
       " 'On',\n",
       " 'English',\n",
       " 'badly',\n",
       " 'dubbed.',\n",
       " 'So',\n",
       " 'found',\n",
       " 'myself',\n",
       " 'wishing',\n",
       " 'AND',\n",
       " 'less',\n",
       " 'time!',\n",
       " '(funny',\n",
       " 'how',\n",
       " 'access',\n",
       " 'graveyard',\n",
       " 'mortuary',\n",
       " 'town)',\n",
       " 'lurid',\n",
       " '(where',\n",
       " 'would',\n",
       " '70s',\n",
       " 'exploitationer',\n",
       " 'without',\n",
       " 'our',\n",
       " 'gratuitous',\n",
       " 'lesbian',\n",
       " 'scene?).',\n",
       " 'mention',\n",
       " '\"romantic\"',\n",
       " 'aspect',\n",
       " '(oh,',\n",
       " 'sweet!)...Miss',\n",
       " '(*)',\n",
       " 'horror',\n",
       " 'weirdest',\n",
       " 'seen.<br',\n",
       " '/>I',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'child',\n",
       " 'abuse',\n",
       " 'vampirism,',\n",
       " 'supposed',\n",
       " 'based',\n",
       " 'upon',\n",
       " 'true',\n",
       " 'character.<br',\n",
       " '/>Our',\n",
       " 'deaf',\n",
       " 'mute',\n",
       " 'result',\n",
       " 'repeated',\n",
       " 'beatings',\n",
       " 'father.',\n",
       " 'doll',\n",
       " 'fetish,',\n",
       " 'cannot',\n",
       " 'figure',\n",
       " 'where',\n",
       " 'from.',\n",
       " 'co-workers',\n",
       " 'tease',\n",
       " 'terribly.<br',\n",
       " '/>During',\n",
       " 'day',\n",
       " 'mild-manner',\n",
       " 'accountant,',\n",
       " 'night',\n",
       " 'breaks',\n",
       " 'cemeteries',\n",
       " 'funeral',\n",
       " 'homes',\n",
       " 'drinks',\n",
       " 'blood',\n",
       " 'girls.',\n",
       " 'They',\n",
       " 'course,',\n",
       " \"wouldn't\",\n",
       " 'fact',\n",
       " 'usually',\n",
       " 'tears',\n",
       " 'clothing',\n",
       " 'down',\n",
       " 'waist.',\n",
       " 'He',\n",
       " 'graduates',\n",
       " 'eventually',\n",
       " 'actually',\n",
       " 'killing,',\n",
       " 'gets',\n",
       " 'caught.<br',\n",
       " '/>Like',\n",
       " 'said,',\n",
       " 'strange',\n",
       " 'dark',\n",
       " 'slow',\n",
       " 'Werner',\n",
       " 'Pochath',\n",
       " 'never',\n",
       " 'talks',\n",
       " 'spends',\n",
       " 'drinking',\n",
       " 'blood.',\n",
       " 'Being',\n",
       " 'long-time',\n",
       " 'fan',\n",
       " 'Japanese',\n",
       " 'expected',\n",
       " 'this.',\n",
       " \"can't\",\n",
       " 'bothered',\n",
       " 'much,',\n",
       " 'poor.',\n",
       " 'might',\n",
       " 'cutest',\n",
       " 'romantic',\n",
       " 'little',\n",
       " 'something',\n",
       " 'ever,',\n",
       " 'pity',\n",
       " \"couldn't\",\n",
       " 'stand',\n",
       " 'awful',\n",
       " 'mess',\n",
       " 'called',\n",
       " 'pacing,',\n",
       " 'standard',\n",
       " '\"quirky\"',\n",
       " 'story.',\n",
       " 'If',\n",
       " \"you've\",\n",
       " 'noticed',\n",
       " 'use',\n",
       " 'characters,',\n",
       " 'plots',\n",
       " 'seem',\n",
       " '\"different\",',\n",
       " 'forcedly',\n",
       " 'so,',\n",
       " 'steer',\n",
       " 'clear',\n",
       " 'Seriously,',\n",
       " '12-year',\n",
       " 'old',\n",
       " 'move',\n",
       " 'along,',\n",
       " \"that's\",\n",
       " 'book.<br',\n",
       " '/>Fans',\n",
       " '\"Beat\"',\n",
       " 'Takeshi:',\n",
       " 'part',\n",
       " 'cameo,',\n",
       " 'unless',\n",
       " \"you're\",\n",
       " 'rabid',\n",
       " 'fan,',\n",
       " 'need',\n",
       " 'suffer',\n",
       " 'film.<br',\n",
       " '/>2/10',\n",
       " '\"Tokyo',\n",
       " 'Eyes\"',\n",
       " '17',\n",
       " 'year',\n",
       " 'girl',\n",
       " 'man',\n",
       " 'being',\n",
       " 'hunted',\n",
       " 'her',\n",
       " 'bro',\n",
       " 'cop.',\n",
       " 'lame',\n",
       " 'flick',\n",
       " '50%',\n",
       " 'filler',\n",
       " 'talk,',\n",
       " 'talk.',\n",
       " \"You'll\",\n",
       " 'stellar',\n",
       " 'cast',\n",
       " 'three',\n",
       " 'talk',\n",
       " 'bus,',\n",
       " 'video',\n",
       " 'games,',\n",
       " 'haircut,',\n",
       " 'walk',\n",
       " 'cell',\n",
       " 'phones,',\n",
       " 'hang',\n",
       " 'etc.',\n",
       " 'read',\n",
       " 'subtitles',\n",
       " 'waiting',\n",
       " 'happen.',\n",
       " 'thin',\n",
       " 'wisp',\n",
       " 'sufficient',\n",
       " 'support',\n",
       " 'low',\n",
       " 'end',\n",
       " 'production',\n",
       " 'value,',\n",
       " 'meager',\n",
       " 'cast,',\n",
       " 'action,',\n",
       " 'romance,',\n",
       " 'sex',\n",
       " 'nudity,',\n",
       " 'heavy',\n",
       " 'drama...just',\n",
       " 'incessant',\n",
       " \"yadayadayada'ing.\",\n",
       " '(C-)',\n",
       " 'Wealthy',\n",
       " 'horse',\n",
       " 'ranchers',\n",
       " 'Buenos',\n",
       " 'Aires',\n",
       " 'long-standing',\n",
       " 'no-trading',\n",
       " 'policy',\n",
       " 'Crawfords',\n",
       " 'Manhattan,',\n",
       " 'happens',\n",
       " 'mustachioed',\n",
       " 'Latin',\n",
       " 'son',\n",
       " 'certain',\n",
       " 'Crawford',\n",
       " 'bright',\n",
       " 'eyes,',\n",
       " 'blonde',\n",
       " 'hair,',\n",
       " 'perky',\n",
       " 'moves',\n",
       " 'dance',\n",
       " 'floor?',\n",
       " 'Century-Fox',\n",
       " 'musical',\n",
       " 'glossy',\n",
       " 'veneer',\n",
       " 'yet',\n",
       " 'seems',\n",
       " 'bit',\n",
       " 'tatty',\n",
       " 'edges.',\n",
       " 'It',\n",
       " 'frenetic,',\n",
       " 'gymnastic-like',\n",
       " 'dancing,',\n",
       " 'exceedingly',\n",
       " 'Betty',\n",
       " 'Grable',\n",
       " '(an',\n",
       " 'eleventh',\n",
       " 'replacement',\n",
       " 'Alice',\n",
       " 'Faye)',\n",
       " 'gives',\n",
       " 'boost,',\n",
       " 'though',\n",
       " \"she's\",\n",
       " 'paired',\n",
       " 'leaden',\n",
       " 'Don',\n",
       " 'Ameche',\n",
       " '(in',\n",
       " 'tan',\n",
       " 'make-up',\n",
       " 'slick',\n",
       " 'hair).',\n",
       " 'Also',\n",
       " 'good:',\n",
       " 'Charlotte',\n",
       " 'Greenwood',\n",
       " \"Betty's\",\n",
       " 'pithy',\n",
       " 'aunt,',\n",
       " 'limousine',\n",
       " 'driver',\n",
       " \"who's\",\n",
       " 'constantly',\n",
       " 'asleep',\n",
       " 'job,',\n",
       " 'Carmen',\n",
       " 'Miranda',\n",
       " 'herself',\n",
       " '(who',\n",
       " 'else?).',\n",
       " 'stock',\n",
       " 'shots',\n",
       " 'Argentina',\n",
       " 'outclass',\n",
       " 'filmed',\n",
       " 'Fox',\n",
       " 'backlot,',\n",
       " 'supporting',\n",
       " 'performances',\n",
       " 'quite',\n",
       " 'awful.',\n",
       " 'By',\n",
       " 'horserace',\n",
       " 'finale,',\n",
       " 'most',\n",
       " 'will',\n",
       " 'enough.',\n",
       " '*1/2',\n",
       " '****',\n",
       " 'Cage',\n",
       " 'drunk',\n",
       " 'critically',\n",
       " 'praise.',\n",
       " 'Elizabeth',\n",
       " 'Shue',\n",
       " 'Actually',\n",
       " 'unattractive',\n",
       " 'overrated',\n",
       " 'piece',\n",
       " 'dung',\n",
       " 'flesh',\n",
       " 'Hollywood.',\n",
       " 'literally',\n",
       " 'vomited',\n",
       " 'film.',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'flu,',\n",
       " 'mean',\n",
       " 'contribute',\n",
       " 'vomit',\n",
       " 'kamode.',\n",
       " '<br',\n",
       " '/>Why',\n",
       " 'Nick',\n",
       " 'pull',\n",
       " 'bad',\n",
       " 'actor.',\n",
       " 'brilliant',\n",
       " 'role',\n",
       " 'Heck',\n",
       " 'nobody',\n",
       " 'better.<br',\n",
       " 'search',\n",
       " 'begins',\n",
       " \"Nick's\",\n",
       " 'contract',\n",
       " 'Lucifer',\n",
       " 'Lou',\n",
       " 'Cipher',\n",
       " '\"Night',\n",
       " 'Train',\n",
       " 'To',\n",
       " 'Terror\".']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00156634 -0.00057168 -0.00056892  0.00391186  0.00436611  0.00322857\n",
      " -0.00285753  0.00339262 -0.00220805 -0.0044377  -0.00116733 -0.00061375\n",
      " -0.00084118  0.00282253 -0.00306073  0.000721   -0.0030427  -0.00392674\n",
      "  0.00339725 -0.00382695  0.0028149  -0.00345134  0.00334856 -0.00295137\n",
      "  0.00306975 -0.00414636  0.00220857  0.00433798 -0.00164621 -0.00213806\n",
      "  0.00295529 -0.0021785  -0.00255844  0.00091168  0.00101862  0.00272277\n",
      "  0.00301214 -0.00261265 -0.00067677  0.00396885  0.00278577 -0.00067683\n",
      " -0.00230345 -0.004754   -0.00036882  0.00427084  0.00039569  0.00404209\n",
      "  0.00088599 -0.00014652  0.00323671  0.00173736 -0.00414521 -0.00461659\n",
      " -0.00116143 -0.00495172 -0.00017505  0.00150349  0.00447745 -0.00216613\n",
      "  0.00084042  0.00191833  0.00112578 -0.00336055  0.00395462 -0.0004212\n",
      " -0.00318817 -0.00248725  0.00030055 -0.00273889 -0.00323885 -0.00201435\n",
      " -0.00131316 -0.00255054 -0.00454028 -0.00400675 -0.00124169 -0.00212296\n",
      "  0.00267673  0.00488541 -0.0041331   0.00363043 -0.00127909 -0.00431965\n",
      " -0.00415602 -0.00324364 -0.00418318  0.00271101 -0.00013538 -0.00052457\n",
      " -0.00269474  0.00315407 -0.00091102 -0.00413479  0.00208103 -0.00159392\n",
      " -0.00390562 -0.00083083  0.0032519   0.00098212]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsun04\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# access vector for one word\n",
    "print(model['watch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "      <th>tokenzied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "      <td>[Once, again, Mr., Costner, has, dragged, out,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "      <td>[This, is, an, example, of, why, the, majority...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "      <td>[First, of, all, I, hate, those, moronic, rapp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "\n",
       "          file                                          tokenzied  \n",
       "0      0_2.txt  [Once, again, Mr., Costner, has, dragged, out,...  \n",
       "1  10000_4.txt  [This, is, an, example, of, why, the, majority...  \n",
       "2  10001_1.txt  [First, of, all, I, hate, those, moronic, rapp...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text):\n",
    "    return np.array([model[x] for x in text])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsun04\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# apply the preprocess function to all reviews\n",
    "df['vec_text'] = df['tokenzied'].apply(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vec_text'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0027712 , -0.00352015, -0.00263619, ...,  0.00078959,\n",
       "        -0.00291353,  0.00162354],\n",
       "       [ 0.00293399,  0.00423996,  0.00057422, ..., -0.00442938,\n",
       "        -0.00162873, -0.00278063],\n",
       "       [-0.00201099,  0.00433001,  0.003913  , ...,  0.00496293,\n",
       "        -0.00131217,  0.00086428],\n",
       "       ...,\n",
       "       [ 0.00052247, -0.00117149, -0.00333994, ..., -0.00072355,\n",
       "        -0.00052909, -0.00175135],\n",
       "       [ 0.00342983, -0.00228086,  0.00462438, ..., -0.00275732,\n",
       "        -0.00103558, -0.0016434 ],\n",
       "       [-0.00175752, -0.00313165,  0.00180346, ...,  0.00290053,\n",
       "         0.00070109, -0.00389416]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vec_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vec_text'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent_vec']=list(map(lambda x:x.sum(axis=0),df.vec_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [0.09312847, -0.094630785, 0.22852522, 0.05958...\n",
       "1     [0.09245081, -0.09774694, 0.13850689, 0.094530...\n",
       "2     [0.06079231, -0.13670747, 0.2628679, -0.002581...\n",
       "3     [0.08619446, -0.13721795, 0.23525174, -0.03831...\n",
       "4     [0.119398855, -0.14799763, 0.18988867, 0.04880...\n",
       "5     [0.017543841, -0.03240566, 0.10874356, 0.03739...\n",
       "6     [-0.015945364, -0.036358036, 0.18017143, 0.004...\n",
       "7     [0.045363635, -0.05529844, 0.11734583, 0.03458...\n",
       "8     [0.06777747, -0.08085461, 0.13584462, 0.001552...\n",
       "9     [-0.0030489878, -0.050781287, 0.07091034, -0.0...\n",
       "10    [0.055961557, -0.047435254, 0.055158243, 0.017...\n",
       "Name: sent_vec, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sent_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sent_vec'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df['sent_vec'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093128</td>\n",
       "      <td>-0.094631</td>\n",
       "      <td>0.228525</td>\n",
       "      <td>0.059582</td>\n",
       "      <td>-0.066349</td>\n",
       "      <td>0.108213</td>\n",
       "      <td>-0.069594</td>\n",
       "      <td>0.037511</td>\n",
       "      <td>-0.042918</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124980</td>\n",
       "      <td>0.115464</td>\n",
       "      <td>-0.010149</td>\n",
       "      <td>-0.020738</td>\n",
       "      <td>0.072868</td>\n",
       "      <td>-0.098510</td>\n",
       "      <td>0.094066</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>-0.097372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.092451</td>\n",
       "      <td>-0.097747</td>\n",
       "      <td>0.138507</td>\n",
       "      <td>0.094530</td>\n",
       "      <td>-0.017681</td>\n",
       "      <td>0.089813</td>\n",
       "      <td>-0.092549</td>\n",
       "      <td>0.036472</td>\n",
       "      <td>-0.021062</td>\n",
       "      <td>-0.058262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060163</td>\n",
       "      <td>0.191369</td>\n",
       "      <td>0.116722</td>\n",
       "      <td>-0.043540</td>\n",
       "      <td>0.039515</td>\n",
       "      <td>-0.146225</td>\n",
       "      <td>-0.011857</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.030818</td>\n",
       "      <td>-0.090125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.060792</td>\n",
       "      <td>-0.136707</td>\n",
       "      <td>0.262868</td>\n",
       "      <td>-0.002581</td>\n",
       "      <td>-0.056489</td>\n",
       "      <td>0.181052</td>\n",
       "      <td>-0.137023</td>\n",
       "      <td>-0.051748</td>\n",
       "      <td>0.067173</td>\n",
       "      <td>-0.002639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066746</td>\n",
       "      <td>0.240909</td>\n",
       "      <td>0.106708</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>0.101189</td>\n",
       "      <td>-0.132757</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>-0.061532</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>-0.047295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086194</td>\n",
       "      <td>-0.137218</td>\n",
       "      <td>0.235252</td>\n",
       "      <td>-0.038314</td>\n",
       "      <td>-0.045759</td>\n",
       "      <td>0.219932</td>\n",
       "      <td>-0.143653</td>\n",
       "      <td>0.051413</td>\n",
       "      <td>0.009665</td>\n",
       "      <td>-0.110976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016176</td>\n",
       "      <td>0.259386</td>\n",
       "      <td>0.095933</td>\n",
       "      <td>-0.007485</td>\n",
       "      <td>0.089147</td>\n",
       "      <td>-0.194201</td>\n",
       "      <td>-0.001812</td>\n",
       "      <td>-0.078791</td>\n",
       "      <td>-0.058275</td>\n",
       "      <td>-0.114373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.119399</td>\n",
       "      <td>-0.147998</td>\n",
       "      <td>0.189889</td>\n",
       "      <td>0.048805</td>\n",
       "      <td>-0.122236</td>\n",
       "      <td>0.096643</td>\n",
       "      <td>-0.113627</td>\n",
       "      <td>0.122275</td>\n",
       "      <td>-0.056252</td>\n",
       "      <td>-0.035505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014434</td>\n",
       "      <td>0.180051</td>\n",
       "      <td>0.113348</td>\n",
       "      <td>-0.054137</td>\n",
       "      <td>0.097740</td>\n",
       "      <td>-0.058026</td>\n",
       "      <td>0.014517</td>\n",
       "      <td>-0.059281</td>\n",
       "      <td>-0.090645</td>\n",
       "      <td>-0.036854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.017544</td>\n",
       "      <td>-0.032406</td>\n",
       "      <td>0.108744</td>\n",
       "      <td>0.037399</td>\n",
       "      <td>-0.049013</td>\n",
       "      <td>0.022647</td>\n",
       "      <td>-0.017196</td>\n",
       "      <td>-0.031546</td>\n",
       "      <td>-0.013009</td>\n",
       "      <td>0.020487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.097475</td>\n",
       "      <td>0.059172</td>\n",
       "      <td>-0.040099</td>\n",
       "      <td>0.054290</td>\n",
       "      <td>-0.043206</td>\n",
       "      <td>0.058571</td>\n",
       "      <td>-0.045362</td>\n",
       "      <td>0.038615</td>\n",
       "      <td>-0.021555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.015945</td>\n",
       "      <td>-0.036358</td>\n",
       "      <td>0.180171</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>0.009180</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>-0.047563</td>\n",
       "      <td>-0.015972</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>-0.029130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144182</td>\n",
       "      <td>0.158345</td>\n",
       "      <td>0.087966</td>\n",
       "      <td>-0.077947</td>\n",
       "      <td>0.031161</td>\n",
       "      <td>-0.180363</td>\n",
       "      <td>0.052373</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.038061</td>\n",
       "      <td>0.021573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.045364</td>\n",
       "      <td>-0.055298</td>\n",
       "      <td>0.117346</td>\n",
       "      <td>0.034580</td>\n",
       "      <td>-0.019521</td>\n",
       "      <td>0.129401</td>\n",
       "      <td>-0.115185</td>\n",
       "      <td>-0.011725</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.027520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056123</td>\n",
       "      <td>0.147282</td>\n",
       "      <td>0.044409</td>\n",
       "      <td>-0.021009</td>\n",
       "      <td>0.084365</td>\n",
       "      <td>-0.072133</td>\n",
       "      <td>0.051046</td>\n",
       "      <td>-0.029510</td>\n",
       "      <td>-0.015222</td>\n",
       "      <td>0.008379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.067777</td>\n",
       "      <td>-0.080855</td>\n",
       "      <td>0.135845</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>-0.004141</td>\n",
       "      <td>0.118693</td>\n",
       "      <td>-0.104664</td>\n",
       "      <td>0.043767</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>-0.008967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106571</td>\n",
       "      <td>0.121945</td>\n",
       "      <td>0.137576</td>\n",
       "      <td>0.034291</td>\n",
       "      <td>0.076052</td>\n",
       "      <td>-0.052747</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>-0.073503</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>-0.085034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.003049</td>\n",
       "      <td>-0.050781</td>\n",
       "      <td>0.070910</td>\n",
       "      <td>-0.048655</td>\n",
       "      <td>-0.062328</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>-0.080659</td>\n",
       "      <td>0.028116</td>\n",
       "      <td>-0.022204</td>\n",
       "      <td>-0.070941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088179</td>\n",
       "      <td>0.088444</td>\n",
       "      <td>0.049958</td>\n",
       "      <td>-0.018261</td>\n",
       "      <td>0.104775</td>\n",
       "      <td>-0.129718</td>\n",
       "      <td>0.068119</td>\n",
       "      <td>-0.054084</td>\n",
       "      <td>-0.020926</td>\n",
       "      <td>0.007447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.055962</td>\n",
       "      <td>-0.047435</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>0.017818</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.101022</td>\n",
       "      <td>-0.103693</td>\n",
       "      <td>0.025308</td>\n",
       "      <td>-0.032865</td>\n",
       "      <td>-0.008872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047928</td>\n",
       "      <td>0.101304</td>\n",
       "      <td>0.060696</td>\n",
       "      <td>-0.016684</td>\n",
       "      <td>0.019749</td>\n",
       "      <td>-0.080905</td>\n",
       "      <td>0.044984</td>\n",
       "      <td>-0.057359</td>\n",
       "      <td>0.032548</td>\n",
       "      <td>-0.052267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.093128 -0.094631  0.228525  0.059582 -0.066349  0.108213 -0.069594   \n",
       "1   0.092451 -0.097747  0.138507  0.094530 -0.017681  0.089813 -0.092549   \n",
       "2   0.060792 -0.136707  0.262868 -0.002581 -0.056489  0.181052 -0.137023   \n",
       "3   0.086194 -0.137218  0.235252 -0.038314 -0.045759  0.219932 -0.143653   \n",
       "4   0.119399 -0.147998  0.189889  0.048805 -0.122236  0.096643 -0.113627   \n",
       "5   0.017544 -0.032406  0.108744  0.037399 -0.049013  0.022647 -0.017196   \n",
       "6  -0.015945 -0.036358  0.180171  0.004195  0.009180  0.104153 -0.047563   \n",
       "7   0.045364 -0.055298  0.117346  0.034580 -0.019521  0.129401 -0.115185   \n",
       "8   0.067777 -0.080855  0.135845  0.001552 -0.004141  0.118693 -0.104664   \n",
       "9  -0.003049 -0.050781  0.070910 -0.048655 -0.062328  0.015332 -0.080659   \n",
       "10  0.055962 -0.047435  0.055158  0.017818  0.004871  0.101022 -0.103693   \n",
       "\n",
       "          7         8         9     ...           90        91        92  \\\n",
       "0   0.037511 -0.042918 -0.017700    ...     0.124980  0.115464 -0.010149   \n",
       "1   0.036472 -0.021062 -0.058262    ...     0.060163  0.191369  0.116722   \n",
       "2  -0.051748  0.067173 -0.002639    ...     0.066746  0.240909  0.106708   \n",
       "3   0.051413  0.009665 -0.110976    ...     0.016176  0.259386  0.095933   \n",
       "4   0.122275 -0.056252 -0.035505    ...     0.014434  0.180051  0.113348   \n",
       "5  -0.031546 -0.013009  0.020487    ...     0.022639  0.097475  0.059172   \n",
       "6  -0.015972  0.012728 -0.029130    ...     0.144182  0.158345  0.087966   \n",
       "7  -0.011725 -0.000977 -0.027520    ...     0.056123  0.147282  0.044409   \n",
       "8   0.043767  0.023022 -0.008967    ...     0.106571  0.121945  0.137576   \n",
       "9   0.028116 -0.022204 -0.070941    ...     0.088179  0.088444  0.049958   \n",
       "10  0.025308 -0.032865 -0.008872    ...     0.047928  0.101304  0.060696   \n",
       "\n",
       "          93        94        95        96        97        98        99  \n",
       "0  -0.020738  0.072868 -0.098510  0.094066  0.039837  0.006789 -0.097372  \n",
       "1  -0.043540  0.039515 -0.146225 -0.011857 -0.014735 -0.030818 -0.090125  \n",
       "2   0.032597  0.101189 -0.132757  0.009021 -0.061532  0.005246 -0.047295  \n",
       "3  -0.007485  0.089147 -0.194201 -0.001812 -0.078791 -0.058275 -0.114373  \n",
       "4  -0.054137  0.097740 -0.058026  0.014517 -0.059281 -0.090645 -0.036854  \n",
       "5  -0.040099  0.054290 -0.043206  0.058571 -0.045362  0.038615 -0.021555  \n",
       "6  -0.077947  0.031161 -0.180363  0.052373  0.003523  0.038061  0.021573  \n",
       "7  -0.021009  0.084365 -0.072133  0.051046 -0.029510 -0.015222  0.008379  \n",
       "8   0.034291  0.076052 -0.052747  0.015969 -0.073503  0.003654 -0.085034  \n",
       "9  -0.018261  0.104775 -0.129718  0.068119 -0.054084 -0.020926  0.007447  \n",
       "10 -0.016684  0.019749 -0.080905  0.044984 -0.057359  0.032548 -0.052267  \n",
       "\n",
       "[11 rows x 100 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=df.merge(X, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "      <th>tokenzied</th>\n",
       "      <th>vec_text</th>\n",
       "      <th>sent_vec</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "      <td>[Once, again, Mr., Costner, has, dragged, out,...</td>\n",
       "      <td>[[0.0027712008, -0.003520147, -0.00263619, -0....</td>\n",
       "      <td>[0.09312847, -0.094630785, 0.22852522, 0.05958...</td>\n",
       "      <td>0.093128</td>\n",
       "      <td>-0.094631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124980</td>\n",
       "      <td>0.115464</td>\n",
       "      <td>-0.010149</td>\n",
       "      <td>-0.020738</td>\n",
       "      <td>0.072868</td>\n",
       "      <td>-0.098510</td>\n",
       "      <td>0.094066</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>-0.097372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "      <td>[This, is, an, example, of, why, the, majority...</td>\n",
       "      <td>[[-0.004457866, 0.00029172233, -0.00085859944,...</td>\n",
       "      <td>[0.09245081, -0.09774694, 0.13850689, 0.094530...</td>\n",
       "      <td>0.092451</td>\n",
       "      <td>-0.097747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060163</td>\n",
       "      <td>0.191369</td>\n",
       "      <td>0.116722</td>\n",
       "      <td>-0.043540</td>\n",
       "      <td>0.039515</td>\n",
       "      <td>-0.146225</td>\n",
       "      <td>-0.011857</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.030818</td>\n",
       "      <td>-0.090125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "      <td>[First, of, all, I, hate, those, moronic, rapp...</td>\n",
       "      <td>[[-0.0026348098, -0.004647377, -0.0045004115, ...</td>\n",
       "      <td>[0.06079231, -0.13670747, 0.2628679, -0.002581...</td>\n",
       "      <td>0.060792</td>\n",
       "      <td>-0.136707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066746</td>\n",
       "      <td>0.240909</td>\n",
       "      <td>0.106708</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>0.101189</td>\n",
       "      <td>-0.132757</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>-0.061532</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>-0.047295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "\n",
       "          file                                          tokenzied  \\\n",
       "0      0_2.txt  [Once, again, Mr., Costner, has, dragged, out,...   \n",
       "1  10000_4.txt  [This, is, an, example, of, why, the, majority...   \n",
       "2  10001_1.txt  [First, of, all, I, hate, those, moronic, rapp...   \n",
       "\n",
       "                                            vec_text  \\\n",
       "0  [[0.0027712008, -0.003520147, -0.00263619, -0....   \n",
       "1  [[-0.004457866, 0.00029172233, -0.00085859944,...   \n",
       "2  [[-0.0026348098, -0.004647377, -0.0045004115, ...   \n",
       "\n",
       "                                            sent_vec         0         1  \\\n",
       "0  [0.09312847, -0.094630785, 0.22852522, 0.05958...  0.093128 -0.094631   \n",
       "1  [0.09245081, -0.09774694, 0.13850689, 0.094530...  0.092451 -0.097747   \n",
       "2  [0.06079231, -0.13670747, 0.2628679, -0.002581...  0.060792 -0.136707   \n",
       "\n",
       "     ...           90        91        92        93        94        95  \\\n",
       "0    ...     0.124980  0.115464 -0.010149 -0.020738  0.072868 -0.098510   \n",
       "1    ...     0.060163  0.191369  0.116722 -0.043540  0.039515 -0.146225   \n",
       "2    ...     0.066746  0.240909  0.106708  0.032597  0.101189 -0.132757   \n",
       "\n",
       "         96        97        98        99  \n",
       "0  0.094066  0.039837  0.006789 -0.097372  \n",
       "1 -0.011857 -0.014735 -0.030818 -0.090125  \n",
       "2  0.009021 -0.061532  0.005246 -0.047295  \n",
       "\n",
       "[3 rows x 108 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df.iloc[:,final_df.columns !='review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df.iloc[:,final_df.columns !='tokenzied']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df.iloc[:,final_df.columns !='vec_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df.iloc[:,final_df.columns !='sent_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 104)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "      <td>0.093128</td>\n",
       "      <td>-0.094631</td>\n",
       "      <td>0.228525</td>\n",
       "      <td>0.059582</td>\n",
       "      <td>-0.066349</td>\n",
       "      <td>0.108213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124980</td>\n",
       "      <td>0.115464</td>\n",
       "      <td>-0.010149</td>\n",
       "      <td>-0.020738</td>\n",
       "      <td>0.072868</td>\n",
       "      <td>-0.098510</td>\n",
       "      <td>0.094066</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>-0.097372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "      <td>0.092451</td>\n",
       "      <td>-0.097747</td>\n",
       "      <td>0.138507</td>\n",
       "      <td>0.094530</td>\n",
       "      <td>-0.017681</td>\n",
       "      <td>0.089813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060163</td>\n",
       "      <td>0.191369</td>\n",
       "      <td>0.116722</td>\n",
       "      <td>-0.043540</td>\n",
       "      <td>0.039515</td>\n",
       "      <td>-0.146225</td>\n",
       "      <td>-0.011857</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.030818</td>\n",
       "      <td>-0.090125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "      <td>0.060792</td>\n",
       "      <td>-0.136707</td>\n",
       "      <td>0.262868</td>\n",
       "      <td>-0.002581</td>\n",
       "      <td>-0.056489</td>\n",
       "      <td>0.181052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066746</td>\n",
       "      <td>0.240909</td>\n",
       "      <td>0.106708</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>0.101189</td>\n",
       "      <td>-0.132757</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>-0.061532</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>-0.047295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type label         file         0         1         2  \\\n",
       "0           0  test   neg      0_2.txt  0.093128 -0.094631  0.228525   \n",
       "1           1  test   neg  10000_4.txt  0.092451 -0.097747  0.138507   \n",
       "2           2  test   neg  10001_1.txt  0.060792 -0.136707  0.262868   \n",
       "\n",
       "          3         4         5    ...           90        91        92  \\\n",
       "0  0.059582 -0.066349  0.108213    ...     0.124980  0.115464 -0.010149   \n",
       "1  0.094530 -0.017681  0.089813    ...     0.060163  0.191369  0.116722   \n",
       "2 -0.002581 -0.056489  0.181052    ...     0.066746  0.240909  0.106708   \n",
       "\n",
       "         93        94        95        96        97        98        99  \n",
       "0 -0.020738  0.072868 -0.098510  0.094066  0.039837  0.006789 -0.097372  \n",
       "1 -0.043540  0.039515 -0.146225 -0.011857 -0.014735 -0.030818 -0.090125  \n",
       "2  0.032597  0.101189 -0.132757  0.009021 -0.061532  0.005246 -0.047295  \n",
       "\n",
       "[3 rows x 104 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
